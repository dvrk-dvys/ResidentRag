# ğŸ§ ğŸ¥ Medical RAG Assistant - Detailed Implementation Plan
**LLM Zoomcamp Final Project - Target: 18+ Points! ğŸ¯**

---

## ğŸ”§ **Tool Overview & Usage Guide**

### ğŸ” **Search Technologies (Qdrant-First Strategy)**
- **ğŸ¯ Qdrant (PRIMARY):** Vector/semantic search with medical embeddings - YOUR FAVORITE! 
- **ğŸ”¤ Elasticsearch (SECONDARY):** BM25 text search for exact medical terminology
- **ğŸ”€ Hybrid Search:** Qdrant-led combination with ES support for best results

### ğŸ§  **LLM & Evaluation**
- **ğŸ¤– OpenAI:** GPT-4o-mini (fast/cheap) vs GPT-4o (powerful/expensive)
- **ğŸ“ Multiple Prompts:** Basic Q&A, context-aware, chain-of-thought
- **ğŸ“Š Evaluation:** Hit Rate, MRR, LLM response quality comparison

### ğŸ’¾ **Data Storage**
- **ğŸ˜ PostgreSQL:** User feedback, metrics, application data
- **ğŸ“ JSON Files:** Medical seed data, processed documents

### ğŸ“Š **Monitoring Stack**
- **ğŸ¯ Prometheus:** Collect app metrics (response times, query counts)
- **ğŸ“ˆ Grafana:** Dashboard with 5+ charts from Prometheus data

---

- **BM25** : classic keyword relevance. Great for exact terminology and phrases.

- **Dense vector (kNN)** : semantic similarity via embeddings.

- **Cosine + normalization** : you normalize vectors because cosine cares about direction, not length.

- **RRF (Reciprocal Rank Fusion)** : simple, strong way to merge ranked lists by ID.

- **source_type** : your lever for filtering/slicing results (wikipedia/textbook/pubmed).

- **title boost (title^2)** : makes matches in titles count more than body text.


## ğŸš€ **Day Zero Status (TODAY - Sept 2, 2025)**

### âœ… **COMPLETED** 
- ğŸ‰ **MedRAG Dataset Successfully Downloaded** â†’ `scripts/create_medical_seed.py`
- ğŸ³ **Docker Compose Infrastructure Setup** â†’ `docker-compose.yml`
- ğŸ“¦ **Dependencies Defined** â†’ `requirements.txt`, `Pipfile`, `Pipfile.lock`
- ğŸ”§ **Test Docker Stack** â†’ `docker-compose up -d` â†’ **Tools: Docker, curl**
- ğŸ **Create Virtual Environment** â†’ `python -m venv venv` â†’ **Tools: Python venv**
- ğŸ“ **Create Project Directory Structure** â†’ **Tools: mkdir, touch**

---

## ğŸ“… **3-Day Sprint with Specific Tools**

### ğŸ—ï¸ **Day 1: Foundation & Multi-Search Pipeline**
*Goal: 3 working search methods + basic RAG*

#### **Morning (9-12pm): Infrastructure & Data**
- ğŸ³ **Start Docker Stack** â†’ `docker-compose up -d`
  - **Tools:** Docker Compose
- **Test:**
  - Elastic Search: http://localhost:9200/
  - QDrant: http://localhost:6333/dashboard
  - Grafana: http://localhost:3000/login

- ğŸ”„ **Index Medical Data into Elasticsearch** 
  - **Tools:** `elasticsearch` Python client, `scripts/es_ingest.py`
  - **Code:** BM25 indexing with medical article fields
- ğŸ¯ **Index Medical Data into Qdrant**
  - **Tools:** `qdrant-client`, `sentence-transformers`
  - **Code:** Generate embeddings, store vectors with metadata

#### **Afternoon (1-5pm): Triple Search Implementation (Qdrant-First!)** 
- ğŸ¯ **Method 1: Pure Qdrant Vector Search (PRIMARY)**
  - **Tools:** `qdrant-client`, `sentence-transformers` 
  - **Code:** `src/search/qdrant_search.py` - medical semantic similarity
- ğŸ”¤ **Method 2: Pure Text Search (ES BM25)**
  - **Tools:** `elasticsearch` client
  - **Code:** `src/search/text_search.py` - medical terminology matching
- ğŸ”€ **Method 3: Qdrant + ES Hybrid (WINNING COMBO)**  
  - **Tools:** Qdrant (primary) + ES (secondary), RRF combination
  - **Code:** `src/search/hybrid_search.py` - Qdrant-led hybrid with ES support

---
ğŸ“ Tiny README blurb (paste-able)

Why Qdrant doesnâ€™t need BM25 here

We use Qdrant for dense, semantic retrieval and Elasticsearch for BM25 keyword retrieval.
Instead of duplicating BM25 in Qdrant, we run a hybrid at the app layer: Qdrant handles concept matching 
(e.g., â€œheart problemsâ€ â†’ â€œcardiac issuesâ€), while Elasticsearch catches exact medical terms, acronyms, and phrases 
(e.g., â€œMIâ€, â€œCOPDâ€, â€œalpha-bisabololâ€). We fuse both result lists with Reciprocal Rank Fusion (RRF),
which consistently outperforms either method alone. This keeps the stack simpler (one BM25 engine)
and maximizes retrieval quality.

âœ… Yes, you should prove Hybrid > single retrievers

For your Retrieval Evaluation (2 pts), compare three modes with the same ground truth:
- qdrant (dense only)
- es_bm25 (BM25 only)
- hybrid (RRF of the two)

You already have wrappers; run Hit@k / MRR on all three and document the results. (You can skip ES kNN if you want â€” Qdrant covers dense.)

â€œso you can RRF-fuse with Elasticsearch laterâ€ â€” what that means

RRF (Reciprocal Rank Fusion) is a simple way to merge two ranked lists. You take top-N IDs from Qdrant and ES,
assign each ID a score like 1/(K+rank), sum scores across lists, and re-rank.
---



#### **Evening (6-9pm): RAG Pipeline**
- ğŸ¤– **OpenAI Integration**
  - **Tools:** `openai` Python client
  - **Code:** `src/llm/openai_client.py`
- ğŸ“ **Basic RAG Flow**
  - **Tools:** Search methods + OpenAI
  - **Code:** `src/llm/rag_pipeline.py` - search â†’ context â†’ LLM â†’ answer
- âœ… **Test All 3 Search Methods**
  - **Tools:** Python scripts, medical questions
  - **Output:** 3 different search results for same query

### ğŸ¨ **Day 2: Evaluation & Interface** 
*Goal: Compare all methods + web interface*

#### **Morning (8-12pm): Retrieval Evaluation (2 POINTS)**
- ğŸ“Š **Create Ground Truth Dataset**
  - **Tools:** Manual curation + GPT generation
  - **Code:** `src/evaluation/ground_truth.py` - 100+ Q&A pairs
- ğŸ“ˆ **Evaluate All 3 Search Methods**
  - **Tools:** Hit Rate, MRR calculations from course
  - **Code:** `src/evaluation/retrieval_eval.py`
  - **Compare:** Pure Qdrant vs Pure ES vs Qdrant+ES Hybrid
  - **Expected Winner:** Qdrant+ES Hybrid (92%+ Hit Rate vs 77% pure vector)
  - **Result:** Documentation of BEST performing method â† **REQUIRED FOR 2 POINTS**

#### **Afternoon (1-5pm): LLM Evaluation (2 POINTS)**  
- ğŸ§  **Test Multiple LLM Approaches**
  - **Tools:** OpenAI API (GPT-4o-mini vs GPT-4o)
  - **Code:** `src/llm/llm_comparison.py`
- ğŸ“ **Test Multiple Prompt Strategies**
  - **Prompt 1:** Basic Q&A template
  - **Prompt 2:** Context-aware with medical focus  
  - **Prompt 3:** Chain-of-thought reasoning
  - **Tools:** `src/llm/prompt_templates.py`
  - **Result:** Documentation of BEST LLM + prompt combo â† **REQUIRED FOR 2 POINTS**

#### **Evening (6-10pm): Streamlit Interface (2 POINTS)**
- ğŸ’¬ **Build Chat Interface** 
  - **Tools:** `streamlit`, session state
  - **Code:** `app/main.py` - chat history, message display
- ğŸ‘ **Add User Feedback System**
  - **Tools:** Streamlit widgets, PostgreSQL
  - **Code:** `src/monitoring/feedback_collector.py` - thumbs up/down, ratings
- ğŸ›ï¸ **Add Search Method Selection**
  - **Tools:** Streamlit selectbox
  - **Feature:** User can choose Text/Vector/Hybrid search

### ğŸ“Š **Day 3: Monitoring & Documentation**
*Goal: Full monitoring + production-ready*

#### **Morning (8-12pm): Monitoring Dashboard (2 POINTS)**
- ğŸ¯ **App Metrics Collection**
  - **Tools:** `prometheus-client` in Python app
  - **Code:** `src/monitoring/metrics_logger.py` - track query times, feedback scores
- ğŸ“ˆ **Grafana Dashboard with 5+ Charts**
  - **Tools:** Grafana UI, Prometheus data source
  - **Charts Required:**
    1. Query volume over time
    2. Response time distribution  
    3. User feedback scores (from PostgreSQL)
    4. Search method usage comparison
    5. Most popular medical topics
  - **Result:** Working dashboard â† **REQUIRED FOR 2 POINTS**

#### **Afternoon (1-5pm): Documentation (2 POINTS)**
- ğŸ“– **Comprehensive README**
  - **Tools:** Markdown, screenshots
  - **Sections:** Setup, usage, evaluation results, architecture
- ğŸ”„ **Reproducible Setup Instructions**
  - **Tools:** Step-by-step Docker commands
  - **Test:** Fresh machine setup verification
  - **Result:** Anyone can run your project â† **REQUIRED FOR 2 POINTS**

#### **Evening (6-9pm): Final Testing & Bonus**
- ğŸŒŸ **Bonus Features Implementation**
  - **Document Re-ranking (1pt):** Cross-encoder model after search
  - **Query Rewriting (1pt):** Improve user queries before search
  - **Hybrid Search (1pt):** Already implemented âœ…
- ğŸŒ **Cloud Deployment (2pts):** Deploy to AWS/GCP if time permits

---

## ğŸ¯ **Scoring Breakdown with Tools**

### âœ… **Core 18 Points - Tool Mapping**
1. **Problem Description (2pts)** â†’ Markdown documentation
2. **Retrieval Flow (2pts)** â†’ Elasticsearch + Qdrant + OpenAI integration  
3. **Retrieval Evaluation (2pts)** â†’ Hit Rate/MRR comparison of 3 search methods
4. **LLM Evaluation (2pts)** â†’ Multiple models + prompt testing
5. **Interface (2pts)** â†’ Streamlit chat application
6. **Ingestion (2pts)** â†’ âœ… MedRAG Python script (automated)
7. **Monitoring (2pts)** â†’ Prometheus + Grafana + user feedback
8. **Containerization (2pts)** â†’ âœ… Docker Compose with all services  
9. **Reproducibility (2pts)** â†’ Clear README + working setup

### ğŸŒŸ **Bonus Points Strategy (5+ pts)**
- **Hybrid Search (1pt)** â†’ âœ… Planned (ES + Qdrant combination)
- **Document Re-ranking (1pt)** â†’ Cross-encoder after initial search
- **Query Rewriting (1pt)** â†’ Query enhancement before search  
- **Cloud Deployment (2pts)** â†’ AWS/GCP deployment
- **Extra Features (1pt)** â†’ Advanced analytics, multi-language support

---

## ğŸ” **When to Use Each Search Technology (Qdrant-First!)**

### ğŸ¯ **Qdrant (PRIMARY - Your Favorite!)**  
**Use for:**
- **Medical concept understanding** â†’ "Heart problems" finds "cardiac issues" 
- **Semantic similarity** â†’ Understanding medical relationships
- **Primary search engine** â†’ Leading the hybrid approach
- **Medical embeddings** â†’ Specialized medical knowledge vectors

### ğŸ”¤ **Elasticsearch (SECONDARY Support)**
**Use for:**
- **Exact medical terminology** â†’ "myocardial infarction" exact matches
- **Acronym searches** â†’ "MI", "COPD", "BP" medical abbreviations  
- **Supporting Qdrant** â†’ Complementing semantic search
- **Fallback search** â†’ When vector search misses obvious keywords

### ğŸ”€ **Qdrant+ES Hybrid (WINNING APPROACH)**
**Best for:**
- **Production medical system** â†’ Qdrant leads, ES supports
- **Highest evaluation scores** â†’ Expected 92%+ Hit Rate (course data shows this)
- **Bonus points** â†’ Advanced Qdrant-centered RAG
- **Your final system** â†’ Qdrant as primary with ES enhancement

---

## ğŸ“Š **Evaluation Strategy for Full Points**

### ğŸ” **Retrieval Evaluation (2 points)**
**Must test ALL THREE approaches:**
1. **Pure Qdrant Vector** â†’ Medical semantic similarity performance (expected ~77%)
2. **Pure ES Text** â†’ Medical terminology matching performance  
3. **Qdrant+ES Hybrid** â†’ Combined approach (expected winner ~92%+)

**Tools:** Hit Rate, MRR metrics â†’ Document Qdrant+ES hybrid as winner â†’ Use in final app

### ğŸ§  **LLM Evaluation (2 points)** 
**Must test MULTIPLE approaches:**
1. **Models:** GPT-4o-mini vs GPT-4o cost/performance
2. **Prompts:** Basic Q&A vs Context-aware vs Chain-of-thought
3. **Settings:** Temperature, max_tokens variations

**Result:** Document best LLM + prompt combination â†’ Use in final app

---

## ğŸ‰ **Success Checklist**

### **Day 1 âœ…**
- [ ] 3 search methods implemented and working
- [ ] Medical data indexed in both ES and Qdrant  
- [ ] Basic RAG pipeline functional
- [ ] All search methods tested with same queries

### **Day 2 âœ…**
- [ ] Ground truth dataset created (100+ medical Q&A)
- [ ] All 3 retrieval methods evaluated and compared
- [ ] Multiple LLM approaches tested and documented
- [ ] Streamlit interface with chat and feedback

### **Day 3 âœ…**
- [ ] Grafana dashboard with 5+ monitoring charts
- [ ] User feedback collection working
- [ ] Comprehensive documentation completed
- [ ] 18+ points verified across all criteria

**ğŸ† Target Score: 21+ points with documented comparisons and best method selection!**