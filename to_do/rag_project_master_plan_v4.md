# ğŸ§ ğŸ¥ Medical RAG Assistant - Updated Implementation Plan v4
**LLM Zoomcamp Final Project - Target: 18+ Points! ğŸ¯**

---

## ğŸš€ **Current Status Update (Sept 4, 2025)**

### âœ… **Day 1 COMPLETED**
- ğŸ³ **Docker Infrastructure** â†’ Docker Compose with Elasticsearch, Qdrant, Grafana running
- ğŸ“¦ **Data Indexing** â†’ Medical data successfully indexed in both Elasticsearch and Qdrant
- ğŸ¯ **Qdrant Vector Search** â†’ Pure semantic search implemented (`src/search/qdrant_search.py`)
- ğŸ”¤ **Elasticsearch BM25** â†’ Pure text search implemented (`src/search/es_search.py`)  
- ğŸ”€ **Hybrid Search RRF** â†’ Qdrant+ES combination with Reciprocal Rank Fusion (`src/search/hybrid_search.py`)
- ğŸ¤– **Basic RAG Pipeline** â†’ OpenAI integration with all 3 search methods working

### ğŸ”„ **Day 1 In Progress**
- ğŸ“Š **Hybrid Search Evaluation** â†’ Currently finishing evaluation of all 3 search methods
- ğŸ“ˆ **Performance Metrics** â†’ Hit Rate and MRR comparisons (hybrid expected to win)

---

## ğŸ“… **Day 2: LangChain Integration & Advanced RAG** 
*Goal: Add LangChain orchestration + auto-expansion capabilities*

### ğŸ§© **Morning (8-12pm): LangChain Integration (NEW)**
**Based on your LangChain notes - implementing the "glue layer" approach:**

- ğŸ”— **Wrap Hybrid Search as LangChain Retriever**
  - **Tools:** `langchain-core`, `langchain-community` 
  - **Code:** `src/search/langchain_hybrid_retriever.py`
  - **Feature:** Convert existing `hybrid_search.py` to LangChain `BaseRetriever` interface
  - **Benefit:** Standardized Document interface, easy composition

**LangChain Setup Instructions:**
```python
# For evaluation (fast, minimal) - stick with es_client.search(...)
hybrid_results = es_client.search(index=index_name, body=hybrid_query)
result_ids = [hit['_source']['id'] for hit in hybrid_results['hits']['hits']]

# For RAG pipeline (Documents + chaining) - use ElasticsearchRetriever  
hybrid_retriever = ElasticsearchRetriever.from_es_params(
    index_name=index_name,
    body_func=hybrid_query,
    content_field='text',
    url=es_url,
)

hybrid_results = hybrid_retriever.invoke(query)
result_docs = []
for hit in hybrid_results:
    result_docs.append(hit.metadata['_source'])
return result_docs
```

**Trade-offs:**
- **Direct es_client.search(...)** â†’ Fast, minimal for metrics (Hit@k/MRR) 
- **ElasticsearchRetriever** â†’ Document objects for RAG chains, more overhead

- ğŸ­ **RAG Chain Composition**
  - **Tools:** `ChatPromptTemplate`, `RunnableLambda`, `RunnablePassthrough`
  - **Code:** `src/llm/langchain_rag_pipeline.py`
  - **Feature:** Clean retrieval â†’ context formatting â†’ LLM chain
  - **Replaces:** Current basic RAG pipeline with composable LangChain version

### ğŸ”„ **Afternoon (1-5pm): Auto-Expansion System (NEW)**
**Implementing the "can't answer â†’ fetch â†’ retry" loop from your notes:**

- ğŸ¤” **Smart Routing Logic**
  - **Tools:** Retrieval confidence scoring, `RunnableBranch`
  - **Code:** `src/llm/expansion_router.py`
  - **Logic:** Check RRF scores + context length â†’ decide if KB expansion needed
  - **Thresholds:** `max_rrf < 0.01` OR `total_chars < 500` triggers expansion

- ğŸ“š **Knowledge Base Auto-Expansion**
  - **Tools:** `WikipediaLoader`, `RecursiveCharacterTextSplitter`
  - **Code:** `src/ingestion/auto_expand_kb.py`  
  - **Flow:** Query â†’ Wikipedia fetch â†’ chunk â†’ upsert to Qdrant+ES â†’ retry search
  - **Benefit:** Automatic knowledge enrichment when existing KB insufficient

- ğŸ” **Retry Pipeline**
  - **Tools:** LangChain routing, your existing hybrid search
  - **Code:** `src/llm/auto_retry_rag.py`
  - **Flow:** 
    1. Try hybrid retrieval 
    2. If low confidence â†’ expand KB with Wikipedia
    3. Retry hybrid search on enriched KB
    4. Answer with enhanced context

### ğŸ“Š **Evening (6-9pm): LangChain Evaluation**
- ğŸ†š **Compare Traditional vs LangChain RAG**
  - **Baseline:** Your current hybrid RAG pipeline  
  - **Enhanced:** LangChain version with auto-expansion
  - **Metrics:** Response quality, knowledge coverage, expansion frequency
- ğŸ“ˆ **Auto-Expansion Effectiveness**
  - **Test:** Questions that trigger Wikipedia expansion
  - **Measure:** Before/after expansion answer quality
  - **Document:** When expansion helps vs when it doesn't

---

## ğŸ“… **Day 2 Continued: Interface & Advanced Features**

### ğŸ¨ **LangChain-Enhanced Streamlit Interface**
- ğŸ’¬ **Enhanced Chat with Auto-Expansion Visibility**
  - **Feature:** Show when Wikipedia expansion triggered
  - **Tools:** Streamlit + LangChain callbacks
  - **Code:** Update `app/main.py` to show expansion events

- ğŸ›ï¸ **Advanced Search Controls**
  - **Feature:** Toggle auto-expansion on/off
  - **Feature:** Choose expansion sources (Wikipedia, HuggingFace, etc.)
  - **Tools:** Streamlit widgets, LangChain tool selection

---

## ğŸ“… **Day 3: Monitoring & Production (UNCHANGED)**
*Goal: Full monitoring + production-ready*

### **Morning (8-12pm): Monitoring Dashboard (2 POINTS)**
- ğŸ“Š **Enhanced Metrics with LangChain**
  - **Additional Metrics:** Expansion trigger rate, Wikipedia fetch times
  - **LangChain:** Use callbacks for tracing retrieval â†’ expansion â†’ retry flows
  - **Tools:** `prometheus-client`, LangChain observability

### **Afternoon (1-5pm): Documentation (2 POINTS)**  
- ğŸ“– **Updated Documentation**
  - **New Section:** LangChain integration benefits and architecture
  - **New Section:** Auto-expansion system explanation  
  - **Updated:** Setup instructions with LangChain dependencies

---

## ğŸ”— **LangChain Integration Benefits (From Your Notes)**

### âœ… **What LangChain Provides:**
- **Standard Interfaces:** `BaseRetriever`, `Document` - your hybrid search plugs in anywhere
- **Clean Composition:** Retrieval â†’ routing â†’ expansion â†’ retry chains
- **Built-in Tools:** Wikipedia loader, text splitters for KB expansion  
- **Observability:** Callbacks and tracing for monitoring expansion events
- **Flexibility:** Easy to add new expansion sources (HuggingFace, PubMed, etc.)

### ğŸ¯ **Your Specific Implementation:**
**Single LLM + Two Tools approach:**
1. **HybridRRFRetriever Tool** â†’ Your proven Qdrant+ES+RRF combination
2. **Expand-KB Tool** â†’ Wikipedia â†’ chunk â†’ upsert â†’ retry

**Flow:**
```
Query â†’ Hybrid Search â†’ Check Confidence â†’ 
  IF Low: Wikipedia Expansion â†’ Re-search â†’ Answer
  IF High: Direct Answer
```

---

## ğŸ¯ **Updated Scoring Strategy (18+ Points)**

### âœ… **Core Requirements (Already Completed/In Progress)**
1. **Problem Description (2pts)** â†’ âœ… Medical RAG with hybrid search
2. **Retrieval Flow (2pts)** â†’ âœ… Elasticsearch + Qdrant + RRF implemented  
3. **Retrieval Evaluation (2pts)** â†’ ğŸ”„ Currently finishing hybrid vs pure comparisons
4. **LLM Evaluation (2pts)** â†’ âœ… Multiple prompts + models tested
5. **Interface (2pts)** â†’ âœ… Streamlit chat with feedback (will enhance with LangChain)
6. **Ingestion (2pts)** â†’ âœ… Automated medical data indexing
7. **Monitoring (2pts)** â†’ âœ… Prometheus + Grafana ready (will add LangChain metrics)
8. **Containerization (2pts)** â†’ âœ… Docker Compose with all services
9. **Reproducibility (2pts)** â†’ âœ… Clear setup (will update for LangChain)

### ğŸŒŸ **Enhanced Bonus Points with LangChain**
- **Hybrid Search (1pt)** â†’ âœ… Already implemented and evaluated
- **Query Enhancement (1pt)** â†’ ğŸ†• LangChain auto-expansion system  
- **Advanced RAG (1pt)** â†’ ğŸ†• LangChain orchestration with retry logic
- **Knowledge Expansion (1pt)** â†’ ğŸ†• Automatic Wikipedia integration
- **Production Features (1pt)** â†’ ğŸ†• LangChain observability and callbacks

---

## ğŸ”§ **Technical Architecture Update**

### **Before LangChain (Day 1 - Completed):**
```
Query â†’ [Qdrant Search | ES Search | Hybrid RRF] â†’ Context â†’ OpenAI â†’ Answer
```

### **After LangChain (Day 2 - New):**
```
Query â†’ LangChain HybridRetriever â†’ Confidence Check â†’ 
  Branch A: Direct Answer (high confidence)
  Branch B: Wikipedia Expansion â†’ Re-retrieve â†’ Answer (low confidence)
```

### **Key Files Added:**
- `src/search/langchain_hybrid_retriever.py` - LangChain wrapper for your hybrid search
- `src/llm/langchain_rag_pipeline.py` - Composable RAG chain  
- `src/ingestion/auto_expand_kb.py` - Wikipedia expansion system
- `src/llm/expansion_router.py` - Confidence-based routing logic
- `src/llm/auto_retry_rag.py` - Complete retry pipeline

---

## ğŸ‰ **Success Criteria Updated**

### **Day 1 âœ… (COMPLETED)**
- [x] 3 search methods implemented and working
- [x] Medical data indexed in both ES and Qdrant  
- [x] Basic RAG pipeline functional
- [ğŸ”„] Hybrid search evaluation in progress

### **Day 2 ğŸ†• (LangChain Focus)**
- [ ] LangChain hybrid retriever wrapper working
- [ ] Auto-expansion system implemented and tested
- [ ] Retry pipeline functional with Wikipedia integration  
- [ ] Enhanced Streamlit interface with expansion visibility
- [ ] Comparison of traditional vs LangChain RAG approaches

### **Day 3 âœ… (Enhanced Monitoring)**
- [ ] Grafana dashboard with LangChain metrics
- [ ] Documentation updated with LangChain architecture
- [ ] 20+ points achieved with advanced features

**ğŸ† Updated Target Score: 22+ points with LangChain auto-expansion system!**

---

## ğŸ§  **Key Implementation Notes from Your LangChain Research**

1. **Keep It Simple:** LangChain as "glue layer" - your hybrid search stays the same, just wrapped
2. **Single LLM Approach:** One model with two tools (retriever + expander) rather than multiple LLMs  
3. **Confidence-Based Routing:** Use retrieval scores and context length to trigger expansion
4. **Modular Design:** Each piece (retriever, expander, router) works independently
5. **Production Ready:** LangChain provides observability and standardization for production use

This v4 plan positions LangChain as the orchestration layer that makes your already-working hybrid search more intelligent and self-improving through automatic knowledge expansion.